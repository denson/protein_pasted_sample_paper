#!/home/denson/anaconda2/bin/python
# -*- coding: utf-8 -*-
"""
Created on Sat May 24 20:43:39 2014

@author: densonsmith

This runs an random_forst, extra tree or SGD classifier on datasets with features generated by SPINE-D
with the SPINE-D prediction as feature #1
"""
import gc
import glob



#import logging

import matplotlib

# Force matplotlib to not use any Xwindows backend.
matplotlib.use('Agg',warn = False)
print("backend")
print(matplotlib.get_backend())

import numpy as np
import pandas as pd
import os



from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import RandomForestClassifier

from sklearn.linear_model import SGDClassifier

from sklearn.metrics import matthews_corrcoef


import pylab as pl


import time


from sklearn.externals import joblib
import cPickle as pickle

from ET_util_funcs import *


import random


            

# set the proper path separator for this operating system
path_sep = os.sep
# get the path to the script  so we can use relative paths
    
# this tells the us where the script is located on the disk
full_path = os.path.realpath(__file__)
script_path, file_path = os.path.split(full_path)

# this adds the correct path seperator for this OS at the end of the path
script_path = script_path + path_sep

# this gets the name of the script that is running. Often it is useful to save
# this in our output so we know which version created the output
script_version = file_path.split('.')[0]

   
    
# keep up with total time
master_start_time = time.time()

# set up garbage collection debugging
#gc.set_debug(gc.DEBUG_UNCOLLECTABLE|gc.DEBUG_COLLECTABLE)

gc.collect()

#logger = logging.getLogger('ET_test')



#This is the column that contains the indicators of the beginning and end of sequences
# -1 is the beginning and 1 is the end of each sequence
#model_list = ["ET", "RDF"]
#criterion_list = ['entropy','gini']


# 'ET', 'RDF' or 'SGD'
model = "ET"
criterion = 'entropy'


full_run = True
# this is the number of features in the window size 1 file
number_of_features = 56
terminal_ID_col = 56
# window size.  We have (window_size - 1) rows before and the same number of rows after each amino acid
if full_run:
    window_size = 31
    forest_size = 1000
else:
    window_size = 1
    forest_size = 10
    
    
# we will randomly pick the parameters from these values
criterion_list = ['gini','entropy']
max_depth_list = [30,40, None]
max_features_list = ['auto', None]
min_samples_split_list = [2,3,4,5,6]
min_samples_leaf_list = [1,1,1,1,3,4,5,6]
bootstrap_list = [False,False,False,True]



windowed_bites_input_path = script_path + 'windowed_bites_files_window_' + np.str(window_size) + path_sep

windowed_bites_ouput_path = script_path + 'diverse_windowed_bites_results_window_'+ np.str(window_size) + path_sep

check_directory(windowed_bites_ouput_path)

windowed_bites_summary_path = script_path + 'diverse_windowed_bites_summary_window_' + np.str(window_size) + path_sep

check_directory(windowed_bites_summary_path)
    
    
max_features = None


# get all the file names
input_filenames = glob.glob(windowed_bites_input_path + "*.csv")

# create the summary dataframe
per_file_performance_headers = ['train file', 'this file MCC', 'ensemble MCC']

per_file_performance_MCC = np.empty(len(input_filenames))
per_file_performance_MCC[:] = np.NaN
empty_strings = np.empty(len(input_filenames))
empty_strings = ''



per_file_performance_df = pd.DataFrame({'train file' : input_filenames, 
                            'this file MCC': per_file_performance_MCC,
                            'ensemble MCC': per_file_performance_MCC,
                            'criterion': empty_strings,
                            'max_depth' : empty_strings,
                            'max_features' : empty_strings,
                            'min_samples_split' : empty_strings,
                            'min_samples_leaf' : empty_strings,
                            'bootstrap' : empty_strings})
                            
                            

# randomize the order of the files

random.shuffle(input_filenames)

        

results_path = script_path + "results_pastes_" + model + \
    "_criterion_" + criterion + "_forest_size_" + np.str(forest_size) + path_sep


    
test_file_name         = "SL329_with_no_spined_feature_windowed_size_31.csv"



# read in training data

test_file_path    = script_path + 'data_sets' + path_sep + test_file_name

test_df = pd.read_csv(test_file_path, header = 0)

rows,cols = np.shape(test_df)

headers = list(test_df.columns.values)

X_headers = headers[1:-1]
y_headers = headers[0]

X_test = test_df[X_headers]
y_test = test_df[y_headers]

test_df = None


# set up the running predicted probas array

running_probas = np.zeros(rows)

# containers to store the parmeters used for each bite



sample_paste_id = 0
for train_file in input_filenames:
    
    this_train_file_name = train_file.split(path_sep)[-1]
    this_train_file_name = this_train_file_name.split('.')[0]
    
    
    print('processing paste sample: %i' % sample_paste_id)
    print('file: %s' % this_train_file_name)
    this_train_df = pd.read_csv(train_file, header = 0)
    
    X_train = this_train_df[X_headers]
    y_train = this_train_df[y_headers]
    
    # clean up
    this_train_df = None
    
    

    
    nrows,ncols = np.shape(X_train)
    #forest_size = 3*ncols
    #forest_size = 500
    #forest_size = ncols 

    
    print "Training the classifier"
    
    nrows,ncol = np.shape(X_train)

    print('n features = %i' % ncols)
    
    # pick the parameters
    criterion_num = np.random.randint(0,len(criterion_list),size=1)[0]
    criterion = criterion_list[criterion_num]

    
    max_depth_num = np.random.randint(0,len(max_depth_list),size=1)[0]
    max_depth = max_depth_list[max_depth_num]

    
    max_features_num = np.random.randint(0,len(max_features_list),size=1)[0]
    max_features = max_features_list[max_features_num]

    
    min_samples_split_num = np.random.randint(0,len(min_samples_split_list),size=1)[0]
    min_samples_split = min_samples_split_list[min_samples_split_num]

    
    min_samples_leaf_num = np.random.randint(0,len(min_samples_leaf_list),size=1)[0]
    min_samples_leaf = min_samples_leaf_list[min_samples_leaf_num]

    bootstrap_num = np.random.randint(0,len(bootstrap_list),size=1)[0]
    bootstrap = bootstrap_list[bootstrap_num]
  




    

    if model == "ET":
        ic = ExtraTreesClassifier(n_estimators=forest_size,
                                  n_jobs = 10,
                                  verbose = 0,
                                  criterion = criterion,
                                  max_depth = max_depth,
                                  max_features = max_features,
                                  min_samples_split = min_samples_split,
                                  min_samples_leaf = min_samples_leaf,
                                  bootstrap = bootstrap)

    elif model == 'RDF':
        ic = RandomForestClassifier(n_estimators=forest_size,
                                  n_jobs = 10,
                                  verbose = 0,
                                  criterion = criterion,
                                  max_depth = max_depth,
                                  max_features = max_features,
                                  min_samples_split = min_samples_split,
                                  min_samples_leaf = min_samples_leaf,
                                  bootstrap = bootstrap)   
    elif model == 'SGD':
        ic = clf = SGDClassifier(shuffle=True,loss='log', verbose = 4, n_iter = 10000)
        
    print ic
    
    #fit and transform the training data
    ic = ic.fit(X_train, y_train)
    
    print('predicting')
    
    y_pred = ic.predict(X_test)
    
    probas = ic.predict_proba(X_test)
    
    this_file_MCC = matthews_corrcoef(y_test,y_pred)
    print('this_file_MCC = %.4f' % this_file_MCC)
    

    
    # set up output prediction dataframe
    
    prediction_headers = ['y_pred', 'proba']
    
    prediction_df = pd.DataFrame({'y_true': y_test, 'y_pred': y_pred,'proba':probas[:,1]})
    
    prediction_output_name = this_train_file_name + '_predictions.csv'
    prediction_output_path = windowed_bites_ouput_path + prediction_output_name
    
    prediction_df.to_csv(prediction_output_path, index = False)
    
    # set up to compute running performance
    running_probas = running_probas + probas[:,1]
    
    # compute the mean. we started sample_paste_id with 0 so add one for the
    # number of samples
    mean_probas = running_probas/(sample_paste_id + 1)
    mean_preds = np.zeros(len(mean_probas))
    mean_preds[mean_probas >= 0.5] = 1
    mean_preds[mean_probas < 0.5] = -1
    
    ensemble_MCC = matthews_corrcoef(y_test,mean_preds)
    
    print('ensemble MCC = %.4f' % ensemble_MCC)
    
    per_file_performance_df.iloc[sample_paste_id, per_file_performance_df.columns.get_loc('train file')] = this_train_file_name
    per_file_performance_df.iloc[sample_paste_id,per_file_performance_df.columns.get_loc('this file MCC')] = this_file_MCC
    per_file_performance_df.iloc[sample_paste_id,per_file_performance_df.columns.get_loc('ensemble MCC')] = ensemble_MCC
    per_file_performance_df.iloc[sample_paste_id,per_file_performance_df.columns.get_loc('criterion')] = criterion
    per_file_performance_df.iloc[sample_paste_id,per_file_performance_df.columns.get_loc('max_depth')] = max_depth
    per_file_performance_df.iloc[sample_paste_id,per_file_performance_df.columns.get_loc('max_features')] = max_features
    per_file_performance_df.iloc[sample_paste_id,per_file_performance_df.columns.get_loc('min_samples_leaf')] = min_samples_leaf
    per_file_performance_df.iloc[sample_paste_id,per_file_performance_df.columns.get_loc('min_samples_split')] = min_samples_split
    per_file_performance_df.iloc[sample_paste_id,per_file_performance_df.columns.get_loc('bootstrap')] = bootstrap
    
    headers = ['this file MCC',
               'ensemble MCC',
               'criterion',
               'max_depth',
               'max_features',
               'min_samples_leaf',
               'min_samples_split',
               'bootstrap',
               'train file']
               
    per_file_performance_df = per_file_performance_df[headers]
    
    
    per_file_performance_output_name = 'bites_per_file_performance_window_size_' + np.str(window_size) + '_' + model + '.csv'
    
    per_file_performance_path = windowed_bites_summary_path + per_file_performance_output_name
    
    per_file_performance_df.to_csv(per_file_performance_path)
        
        
    sample_paste_id +=1
        
        
