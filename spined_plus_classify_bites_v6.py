#!/home/denson/anaconda2/bin/python
# -*- coding: utf-8 -*-
'''
Created on Sat May 24 20:43:39 2014

@author: densonsmith

This runs an random_forst, extra tree or SGD classifier on datasets with features generated by SPINE-D
with the SPINE-D prediction as feature #1
'''
import gc
import glob



#import logging

import matplotlib

# Force matplotlib to not use any Xwindows backend.
matplotlib.use('Agg',warn = False)
print('backend')
print(matplotlib.get_backend())

import numpy as np
import pandas as pd
import os



from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import RandomForestClassifier

from sklearn.linear_model import SGDClassifier

from sklearn.ensemble import GradientBoostingClassifier

from sklearn.metrics import matthews_corrcoef


import pylab as pl


import time


from sklearn.externals import joblib
import cPickle as pickle

from ET_util_funcs import *


import random


            

# set the proper path separator for this operating system
path_sep = os.sep
# get the path to the script  so we can use relative paths
    
# this tells the us where the script is located on the disk
full_path = os.path.realpath(__file__)
script_path, file_path = os.path.split(full_path)

# this adds the correct path seperator for this OS at the end of the path
script_path = script_path + path_sep

# this gets the name of the script that is running. Often it is useful to save
# this in our output so we know which version created the output
script_version = file_path.split('.')[0]

   
    
# keep up with total time
master_start_time = time.time()

# set up garbage collection debugging
#gc.set_debug(gc.DEBUG_UNCOLLECTABLE|gc.DEBUG_COLLECTABLE)

gc.collect()

#logger = logging.getLogger('ET_test')



#This is the column that contains the indicators of the beginning and end of sequences
# -1 is the beginning and 1 is the end of each sequence
#model_list = ['ET', 'RDF']
#criterion_list = ['entropy','gini']


# 'ET', 'RDF', 'SGD' or 'GBC'
model = 'GBC'
criterion = 'entropy'


full_run = False


# GBC parameters
# GBC is robust against overfitting so more is almost always better but 
# takes longer
# n_boosts corresponds to the learning rate
n_boosts = [500,1000,2000,5000]
learning_rates = [1.0, 0.1, 0.01, 0.001]
max_depths = [3,4,5]
max_features_list = [None, 'sqrt']

# the sample size of the bites
sample_size = 10
# this is the number of features in the window size 1 file
number_of_features = 56
terminal_ID_col = 56
# window size.  We have (window_size - 1) rows before and the same number of rows after each amino acid
if full_run:
    window_size = 31
    forest_size = 1000
else:
    window_size = 3
    forest_size = 100
    
windowed_bites_input_path = script_path + 'windowed_bites_files_window_'  + \
    np.str(window_size) + '_sample_size_' + np.str(sample_size) + 'percent' + path_sep
windowed_bites_ouput_path = script_path + 'windowed_bites_results_window_' + \
    np.str(window_size) + '_sample_size_' + np.str(sample_size) + 'percent'  \
    + '_' + model + path_sep
    
print('creating results path:')
print(windowed_bites_ouput_path)

check_directory(windowed_bites_ouput_path)

windowed_bites_summary_path = script_path + 'windowed_bites_summary_window_' + \
    np.str(window_size) + '_sample_size_' + np.str(sample_size) + 'percent'  \
    + '_' + model + path_sep
    
    
    
print('creating summary path:')
print(windowed_bites_summary_path)
    
check_directory(windowed_bites_summary_path)
    
    
max_features = None

print windowed_bites_input_path

# get all the file names
input_filenames = glob.glob(windowed_bites_input_path + '*.csv')

# randomize the order of the files

random.shuffle(input_filenames)

# Run on a sample of 20 files

print('number of input bite files = %i' % len(input_filenames))


'''
print('Taking a sample of 20 files')

input_filenames = input_filenames[0:20]

'''

# create the summary dataframe
per_file_performance_headers = ['train file', 'this file MCC', 'ensemble MCC']

per_file_performance_MCC = np.empty(len(input_filenames))
per_file_performance_MCC[:] = np.NaN

per_file_performance_df = pd.DataFrame({'train file' : input_filenames, 
                            'this file MCC': per_file_performance_MCC,
                            'ensemble MCC': per_file_performance_MCC})
                            
                            



        

results_path = script_path + 'results_pastes_' + model + \
    '_criterion_' + criterion + '_forest_size_' + np.str(forest_size) + path_sep


    
test_file_name         = 'SL329_with_no_spined_feature_windowed_size_' + \
    np.str(window_size) + '.csv'



# read in training data

test_file_path    = script_path + 'data_sets' + path_sep + test_file_name

test_df = pd.read_csv(test_file_path, header = 0)

rows,cols = np.shape(test_df)

headers = list(test_df.columns.values)

X_headers = headers[1:-1]
y_headers = headers[0]

X_test = test_df[X_headers]
y_test = test_df[y_headers]

test_df = None


# set up the running predicted probas array

running_probas = np.zeros(rows)



    
sample_paste_id = 0
for train_file in input_filenames:
    
    this_train_file_name = train_file.split(path_sep)[-1]
    this_train_file_name = this_train_file_name.split('.')[0]
    
    
    print('processing paste sample: %i' % sample_paste_id)
    print('file: %s' % this_train_file_name)
    this_train_df = pd.read_csv(train_file, header = 0)
    
    X_train = this_train_df[X_headers]
    y_train = this_train_df[y_headers]
    
    # clean up
    this_train_df = None
    
    

    
    nrows,ncols = np.shape(X_train)
    #forest_size = 3*ncols
    #forest_size = 500
    #forest_size = ncols 

    
    print 'Training the classifier'
    
    nrows,ncol = np.shape(X_train)

    print('n features = %i' % ncols)
    
    
    start_time = time.time()
    if model == 'ET':
        ic = ExtraTreesClassifier(n_estimators=forest_size, 
                criterion=criterion, n_jobs = 10,verbose = 0,max_features=max_features)
    elif model == 'RDF':
        ic = RandomForestClassifier(n_estimators=forest_size, 
                criterion=criterion, n_jobs = 10,verbose = 0,max_features=max_features)     
    elif model == 'SGD':
        ic = SGDClassifier(shuffle=True,loss='log', verbose = 4, n_iter = 10000)
    
    elif model == 'GBC':
        # learning_rates = [1.0, 0.1, 0.01]
        
        roll = np.random.randint(len(learning_rates),size = 1)[0]
        learning_rate = learning_rates[roll]
        this_n_boosts = n_boosts[roll]
    
        # max_depths = [3,4,5]
        
        roll = np.random.randint(len(max_depths),size = 1)[0]
        this_max_depth = max_depths[roll]
    
        
        roll = np.random.randint(len(max_features_list),size = 1)[0]
        max_features = max_features_list[roll]      

         
            
        ic = GradientBoostingClassifier(n_estimators=this_n_boosts, 
                            learning_rate=learning_rate, 
                            max_depth = this_max_depth,
                            max_features = max_features,
                            verbose = 1)
        print ic
        
        gbc_params = ic.get_params(deep=True)
        
        # Make a dataframe of the parameters

    
        if sample_paste_id == 0:
            
        
            keys = gbc_params.keys()
            values = gbc_params.values()
            
            header = ('values bite %i' % sample_paste_id)
            params_df = pd.DataFrame(data=values,index=keys,columns = [header])
            
        else:
            header = ('values bite %i' % sample_paste_id)
            values = gbc_params.values()
            params_df[header] = values
            
        params_output_path = windowed_bites_summary_path + 'bite_model_parameters.csv'
            
        params_df.to_csv(params_output_path,
                         index_label = 'parameter')
        
            
        
    #fit and transform the training data
    ic = ic.fit(X_train, y_train)
    
    print('predicting')
    
    y_pred = ic.predict(X_test)
    
    probas = ic.predict_proba(X_test)
    
    this_file_MCC = matthews_corrcoef(y_test,y_pred)
    print('this_file_MCC = %.4f' % this_file_MCC)
    

    
    # set up output prediction dataframe
    
    prediction_headers = ['y_pred', 'proba']
    
    prediction_df = pd.DataFrame({'y_true': y_test, 'y_pred': y_pred,'proba':probas[:,1]})
    
    prediction_output_name = this_train_file_name + '_predictions.csv'
    prediction_output_path = windowed_bites_ouput_path + prediction_output_name
    
    prediction_df.to_csv(prediction_output_path, index = False)
    
    # set up to compute running performance
    running_probas = running_probas + probas[:,1]
    
    # compute the mean. we started sample_paste_id with 0 so add one for the
    # number of samples
    mean_probas = running_probas/(sample_paste_id + 1)
    mean_preds = np.zeros(len(mean_probas))
    mean_preds[mean_probas >= 0.5] = 1
    mean_preds[mean_probas < 0.5] = -1
    
    ensemble_MCC = matthews_corrcoef(y_test,mean_preds)
    
    print('ensemble MCC = %.4f' % ensemble_MCC)
    
    per_file_performance_df.iloc[sample_paste_id, per_file_performance_df.columns.get_loc('train file')] = this_train_file_name
    per_file_performance_df.iloc[sample_paste_id,per_file_performance_df.columns.get_loc('this file MCC')] = this_file_MCC
    per_file_performance_df.iloc[sample_paste_id,per_file_performance_df.columns.get_loc('ensemble MCC')] = ensemble_MCC
    
    per_file_performance_output_name = 'bites_per_file_performance_window_size_' + np.str(window_size) + '_' + model + '.csv'
    
    per_file_performance_path = windowed_bites_summary_path + per_file_performance_output_name
    
    per_file_performance_df.to_csv(per_file_performance_path, index = False)
        
        
    sample_paste_id +=1
        
        
