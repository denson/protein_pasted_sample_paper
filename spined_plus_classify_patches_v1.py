#!/home/denson/anaconda2/bin/python
# -*- coding: utf-8 -*-
"""
Created on Sat May 24 20:43:39 2014

@author: densonsmith

This runs an random_forst, extra tree or SGD classifier on datasets with features generated by SPINE-D
with the SPINE-D prediction as feature #1
"""
import gc
import glob



#import logging

import matplotlib

# Force matplotlib to not use any Xwindows backend.
matplotlib.use('Agg',warn = False)
print("backend")
print(matplotlib.get_backend())

import numpy as np
import pandas as pd
import os



from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import RandomForestClassifier

from sklearn.linear_model import SGDClassifier

from sklearn.metrics import matthews_corrcoef


import pylab as pl


import time


from sklearn.externals import joblib
import cPickle as pickle

from ET_util_funcs import *


import random


            

# set the proper path separator for this operating system
path_sep = os.sep
# get the path to the script  so we can use relative paths
    
# this tells the us where the script is located on the disk
full_path = os.path.realpath(__file__)
script_path, file_path = os.path.split(full_path)

# this adds the correct path seperator for this OS at the end of the path
script_path = script_path + path_sep

# this gets the name of the script that is running. Often it is useful to save
# this in our output so we know which version created the output
script_version = file_path.split('.')[0]

   
    
# keep up with total time
master_start_time = time.time()

# set up garbage collection debugging
#gc.set_debug(gc.DEBUG_UNCOLLECTABLE|gc.DEBUG_COLLECTABLE)

gc.collect()

#logger = logging.getLogger('ET_test')



#This is the column that contains the indicators of the beginning and end of sequences
# -1 is the beginning and 1 is the end of each sequence
#model_list = ["ET", "RDF"]
#criterion_list = ['entropy','gini']


# 'ET', 'RDF' or 'SGD'
model = "ET"
criterion = 'entropy'


full_run = False
# this is the number of features in the window size 1 file
number_of_features = 56
terminal_ID_col = 56
# window size.  We have (window_size - 1) rows before and the same number of rows after each amino acid
if full_run:
    window_size = 31
    forest_size = 1000
else:
    window_size = 1
    forest_size = 100
    
windowed_patches_input_path = script_path + 'windowed_patches_files_window_' + np.str(window_size) + path_sep
windowed_patches_input_path
print 

windowed_patches_ouput_path = script_path + 'windowed_patches_results_window_' + np.str(window_size) + path_sep

check_directory(windowed_patches_ouput_path)

windowed_patches_summary_path = script_path + 'windowed_patches_summary_window_' + np.str(window_size) + path_sep

check_directory(windowed_patches_summary_path)
    
    
max_features = None


# get all the file names
input_filenames = glob.glob(windowed_patches_input_path + "*.csv")

# create the summary dataframe
per_file_performance_headers = ['train file', 'this file MCC', 'ensemble MCC']

per_file_performance_MCC = np.empty(len(input_filenames))
per_file_performance_MCC[:] = np.NaN

per_file_performance_df = pd.DataFrame({'train file' : input_filenames, 
                            'this file MCC': per_file_performance_MCC,
                            'ensemble MCC': per_file_performance_MCC})
                            
                            

# randomize the order of the files

random.shuffle(input_filenames)

        

results_path = script_path + "results_pastes_" + model + \
    "_criterion_" + criterion + "_forest_size_" + np.str(forest_size) + path_sep


    
test_file_name         = "SL329_with_no_spined_feature_windowed_size_1.csv"



# read in training data

test_file_path    = script_path + 'data_sets' + path_sep + test_file_name

test_df = pd.read_csv(test_file_path, header = 0)

rows,cols = np.shape(test_df)




# set up the running predicted probas array

running_probas = np.zeros(rows)



    
sample_paste_id = 0
for train_file in input_filenames:
    
    this_train_file_name = train_file.split(path_sep)[-1]
    this_train_file_name = this_train_file_name.split('.')[0]
    
    
    print('processing paste sample: %i' % sample_paste_id)
    print('file: %s' % this_train_file_name)
    this_train_df = pd.read_csv(train_file, header = 0)
    
    headers = list(this_train_df.columns.values)
    
    X_headers = headers[1:-1]
    y_headers = headers[0]
    
    X_test = test_df[X_headers]
    y_test = test_df[y_headers]

 
    
    X_train = this_train_df[X_headers]
    y_train = this_train_df[y_headers]
    
    # clean up
    this_train_df = None
    
    

    
    nrows,ncols = np.shape(X_train)
    #forest_size = 3*ncols
    #forest_size = 500
    #forest_size = ncols 

    
    print "Training the classifier"
    
    nrows,ncol = np.shape(X_train)

    print('n features = %i' % ncols)
    
    
    start_time = time.time()
    if model == "ET":
        ic = ExtraTreesClassifier(n_estimators=forest_size, 
                criterion=criterion, n_jobs = 10,verbose = 0,max_features=max_features)
    elif model == 'RDF':
        ic = RandomForestClassifier(n_estimators=forest_size, 
                criterion=criterion, n_jobs = 10,verbose = 0,max_features=max_features)     
    elif model == 'SGD':
        ic = clf = SGDClassifier(shuffle=True,loss='log', verbose = 4, n_iter = 10000)
    
    #fit and transform the training data
    ic = ic.fit(X_train, y_train)
    
    print('predicting')
    
    y_pred = ic.predict(X_test)
    
    probas = ic.predict_proba(X_test)
    
    this_file_MCC = matthews_corrcoef(y_test,y_pred)
    print('this_file_MCC = %.4f' % this_file_MCC)
    

    
    # set up output prediction dataframe
    
    prediction_headers = ['y_pred', 'proba']
    
    prediction_df = pd.DataFrame({'y_true': y_test, 'y_pred': y_pred,'proba':probas[:,1]})
    
    prediction_output_name = this_train_file_name + '_predictions.csv'
    prediction_output_path = windowed_patches_ouput_path + prediction_output_name
    
    prediction_df.to_csv(prediction_output_path, index = False)
    
    # set up to compute running performance
    running_probas = running_probas + probas[:,1]
    
    # compute the mean. we started sample_paste_id with 0 so add one for the
    # number of samples
    mean_probas = running_probas/(sample_paste_id + 1)
    mean_preds = np.zeros(len(mean_probas))
    mean_preds[mean_probas >= 0.5] = 1
    mean_preds[mean_probas < 0.5] = -1
    
    ensemble_MCC = matthews_corrcoef(y_test,mean_preds)
    
    print('ensemble MCC = %.4f' % ensemble_MCC)
    
    per_file_performance_df.iloc[sample_paste_id, per_file_performance_df.columns.get_loc('train file')] = this_train_file_name
    per_file_performance_df.iloc[sample_paste_id,per_file_performance_df.columns.get_loc('this file MCC')] = this_file_MCC
    per_file_performance_df.iloc[sample_paste_id,per_file_performance_df.columns.get_loc('ensemble MCC')] = ensemble_MCC
    
    per_file_performance_output_name = 'patches_per_file_performance_window_size_' + np.str(window_size) + '_' + model + '.csv'
    
    per_file_performance_path = windowed_patches_summary_path + per_file_performance_output_name
    
    per_file_performance_df.to_csv(per_file_performance_path, index = False)
        
        
    sample_paste_id +=1
        
        
