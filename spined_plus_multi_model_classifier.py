#!/home/denson/anaconda2/bin/python
# -*- coding: utf-8 -*-
"""
Created on Sat May 24 20:43:39 2014

@author: densonsmith

This runs an random_forst, extra tree or SGD classifier on datasets with features generated by SPINE-D
with the SPINE-D prediction as feature #1
"""
import gc
import csv
#import logging

import matplotlib

# Force matplotlib to not use any Xwindows backend.
matplotlib.use('Agg',warn = False)
print("backend")
print(matplotlib.get_backend())

import numpy as np
import pandas as pd
import os



from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier

from sklearn.linear_model import SGDClassifier

from sklearn import svm

import pylab as pl


import time


from sklearn.externals import joblib
import cPickle as pickle

from ET_util_funcs import *



            
# get the path to the script  so we can use relative paths
full_path = os.path.realpath(__file__)
script_path, file_name = os.path.split(full_path)
    
   
    
# keep up with total time
master_start_time = time.time()

# set up garbage collection debugging
#gc.set_debug(gc.DEBUG_UNCOLLECTABLE|gc.DEBUG_COLLECTABLE)

gc.collect()

#logger = logging.getLogger('ET_test')



#This is the column that contains the indicators of the beginning and end of sequences
# -1 is the beginning and 1 is the end of each sequence
#model_list = ["ET", "RDF"]
#criterion_list = ['entropy','gini']


# 'ET', 'RDF', 'SGD', 'SVM' or 'GBC'
model_list = ["GBC"]
criterion_list = ['entropy']

# SVM parameters
C = 1.0
gamma = 0.7


# GBC parameters


GBC_max_features = None
GBC_learning_rate = 0.01
GBC_n_boosts = 500
GBC_max_depth = 4
GBC_subsample = 0.01
GBC_verbose = 4



full_run = False
# this is the number of features in the window size 1 file
number_of_features = 56
terminal_ID_col = 56
# window size.  We have (window_size - 1) rows before and the same number of rows after each amino acid
if full_run:
    window_size = 31
    forest_size = 1000
else:
    window_size = 3
    forest_size = 500
    

    
    
max_features = None

for model in model_list:
    for criterion in criterion_list:
        
        if criterion == 'gini':
            #threshold_adjustment = 0.1204
            #threshold_adjustment = 0.3796
            threshold_adjustment = 0.0
        elif criterion == 'entropy':
            #threshold_adjustment = 0.0

            #threshold_adjustment = 0.422 #DM3000 train DM1229 test
            # threshold_adjustment = 0.174 #DM4229 train SL329 test
            #threshold_adjustment = 0.401 # DM4080 Train SL477 test
            threshold_adjustment = 0.0
            
    
        result_path = "/results_best_feature_classifier_model_" + model + \
            "_criterion_" + criterion + "_forest_size_" + np.str(forest_size) + \
            "_theshold_adjustment_" + np.str(threshold_adjustment) + "/"
        
        this_path = script_path + result_path
        check_directory(this_path)
        
        progress_file_name = this_path + "best_feature_classifier_progress_window_" + np.str(window_size) + "_" + criterion + ".txt"

        progress_file = open(progress_file_name, 'w')
        
        # start test on one big dataset and training set
        
        # read in training data
        
        
        #train_file_name = "DM4080_with_spined_feature"
        #train_file_name = "DM3000_with_spined_feature"
        train_file_name = "DM4229_with_no_spined_feature"
        
        train_path    = "/data_sets/"
        
        
        
        #test_file_name_list         = ["SL477_with_spined_feature"]
        #test_file_name_list         = ["DM1229_with_spined_feature"]
        test_file_name_list         = ["SL329_with_no_spined_feature"]
        
        # read in training data
        file_name = train_file_name
        
        file_path    = script_path + "/data_sets/"
        
        
        
        
        
        
        
        # create window array
        
        
        
        print("window size = %d" % window_size)
        outline = "window size =  " + np.str(window_size)
        progress_file.write(outline)
        progress_file.write("\n")  
        progress_file.flush()
                    
        
        try:
            print("Attempting to load saved model.")
        
            dump_name = "best_feature_classifier_saved_window_size_" + model + \
                "_criterion_" + criterion + "_forest_size_" + np.str(forest_size)
                
            dump_path = script_path + "/saved_models/" + dump_name + "_" + train_file_name + "_train/" 
            dump_path = dump_path + dump_name  + ".pkl"
            
            ic = joblib.load(dump_path)
                
            # we need all the features now
            # we are using features that contain 90% of cumulative importances
            '''
            if full_run:
                feature_importance_file = script_path + "/best_feature_importances/" + "best_features_90_percent_threshold_ET_num_features_all_forest_size_100.csv"
            else:
                feature_importance_file = script_path + "/best_feature_importances/" + "best_features_90_percent_threshold_ET_num_features_all_forest_size_100_short.csv"
                
            best_features = np.genfromtxt(feature_importance_file,usecols=0,delimiter = ",")
            best_features = best_features.astype(np.int)
            '''
            
        except:
            
            
            print("No saved model found, loading training data.")
            
            training_data = load_dataset(file_name,file_path)
        
            training_data = generate_window(training_data,window_size,number_of_features,terminal_ID_col)
            
            
            
            # we need all the features now       
            # we are using features that contain 90% of cumulative importances
            '''
            if full_run:
                feature_importance_file = script_path + "/best_feature_importances/" + "best_features_90_percent_threshold_ET_num_features_all_forest_size_100.csv"
            else:
                feature_importance_file = script_path + "/best_feature_importances/" + "best_features_90_percent_threshold_ET_num_features_all_forest_size_100_short.csv"
            
            best_features = np.genfromtxt(feature_importance_file,usecols=0,delimiter = ",")
            best_features = best_features.astype(np.int)
            '''

        
                
            # set up training data      
            
            y_train = training_data[:,0]
            
            X_train = training_data[:,1:]
            
            
            
            
            # clean up
            training_data = []
            
            
            nrows,ncols = np.shape(X_train)
            #forest_size = 3*ncols
            #forest_size = 500
            #forest_size = ncols 

            
            print "Training the classifier"
            print("window size = %d" % window_size)
            outline = "Training the classifier"
            progress_file.write(outline)
            progress_file.write("\n")  
            progress_file.flush()
            
            
            start_time = time.time()
            if model == "ET":
                ic = ExtraTreesClassifier(n_estimators=forest_size, 
                        criterion=criterion, n_jobs = 10,verbose = 2,max_features=max_features)
            elif model == 'RDF':
                ic = RandomForestClassifier(n_estimators=forest_size, 
                        criterion=criterion, n_jobs = 10,verbose = 2,max_features=max_features)     
            elif model == 'SGD':
                ic = clf = SGDClassifier(shuffle=True,loss='log', verbose = 4, n_iter = 10000)
                
            elif model == 'SVM':
                ic = clf = svm.SVC(kernel='rbf', gamma=gamma, C=C)
                
            elif model == 'GBC':  
                max_features = GBC_max_features
                learning_rate = GBC_learning_rate
                max_depth = GBC_max_depth
                subsample = GBC_subsample
                n_estimators = GBC_n_boosts 
                verbose = GBC_verbose
                
                ic = GradientBoostingClassifier(n_estimators=n_estimators, 
                                    learning_rate=learning_rate, 
                                    max_depth = max_depth,
                                    max_features = max_features,
                                    subsample = subsample,
                                    verbose = verbose) 
            
            #fit and transform the training data
            ic = ic.fit(X_train, y_train)
            
        
            
            
            elapsed_time = time.time() - start_time
            print "Time = %f" % elapsed_time
            print
            outline = "Time =  " + np.str(elapsed_time)
            progress_file.write(outline)
            progress_file.write("\n")  
            progress_file.flush()
            
            start_time = time.time()
            print "Saving the classifier"
            print("window size = %d" % window_size)
            print("criterion = %s" % criterion )
            outline = "Saving the classifier"
            progress_file.write(outline)
            progress_file.write("\n")  
            outline = "Window size = " + np.str(window_size)
            progress_file.write(outline)
            progress_file.write("\n")
            outline = "criterion = " + criterion
            progress_file.write(outline)
            progress_file.write("\n")
            
            progress_file.flush()
        
            
            # Uncomment this block to save models
            
            dump_name = "best_feature_classifier_saved_window_size_" + model + \
                "_criterion_" + criterion + "_forest_size_" + np.str(forest_size)
                
            dump_path = script_path + "/saved_models/" + dump_name + "_" + train_file_name + "_train/" 
            check_directory(dump_path)
            
            dump_path = dump_path + dump_name + ".pkl"
            
            
            
            joblib.dump(ic, dump_path)
            
        
            
            
            elapsed_time = time.time() - start_time
            print "Time = %f" % elapsed_time
            print
            outline = "Time =  " + np.str(elapsed_time)
            progress_file.write(outline)
            progress_file.write("\n")  
            progress_file.flush()
            
            
            # clean up
            print("garbage collecting")
            X_train = []
            y_train = []
            
            gc.collect()
        
        for test_file_name in test_file_name_list:
            
            # read in test data  
                    
            file_name = test_file_name
            
            file_path    = script_path + "/data_sets/"
            
            test_data = load_dataset(file_name,file_path)
            
            
            this_path = script_path + result_path + "/plots/"
            check_directory(this_path)
            plot_path = this_path
            
        
            
            # create window array
            
            
            
            test_data = generate_window(test_data,window_size,number_of_features,terminal_ID_col)
                    
            # set up test data
            y_test = test_data[:,0]
            

            X_test = test_data[:,1:]
            
            
            
            
            
            outline = "Predicting " + test_file_name + "_" + model + "_" + criterion
            print(outline) 
            
            progress_file.write(outline)
            progress_file.write("\n")  
            progress_file.flush()
            
            start_time = time.time()
            class_predictions = ic.predict(X_test);
            probas = ic.predict_proba(X_test)
            if model != 'SGD':
                importances = ic.feature_importances_
            else:
                # we have no importance estimate from SGD
                importances = np.ones(ncols)/ncols
                
            elapsed_time = time.time() - start_time
            
            print "Time = %f" % elapsed_time
            print
            outline = "Time =  " + np.str(elapsed_time)
            progress_file.write(outline)    
            progress_file.write("\n")  
            progress_file.flush()
            
            outline = "Performance metrics " + test_file_name + "_" + model + "_" + criterion
            print(outline)
            outline = "Dumping performance metrics"
            progress_file.write(outline) 
            progress_file.write("\n")  
            progress_file.flush()
            
            csv_name = "unadjusted_probas_" + test_file_name  + "_window_size_" + np.str(window_size ) + "_" + model + "_" + criterion 
            fmt = "%.6f, %.6f"
            
 
            
            csv_path = script_path + result_path + csv_name + ".csv"
            np.savetxt(csv_path, probas, fmt=fmt)
            
            csv_name = "unadjusted_predictions_" + test_file_name  + "_window_size_" + np.str(window_size ) + "_" + model + "_" + criterion
            fmt = "%d"
            csv_path = script_path + result_path + csv_name + ".csv"
            np.savetxt(csv_path, class_predictions, fmt=fmt)
            
            csv_name = "y_test_" + test_file_name  + "_window_size_" + np.str(window_size ) + "_" + model + "_" + criterion
            fmt = "%d"
            csv_path = script_path + result_path + csv_name + ".csv"
            np.savetxt(csv_path, y_test, fmt=fmt)    
            
            # todo: add a while loop here to search for threshold that gives
            # a specificity of 0.85.  Then use that threshold for performance
            # metrics and write out the threshold used to a file
            print
            print("searching for threshold")
            plot_name = train_file_name + "_train_" + test_file_name + "_" + np.str(window_size) + "_" + model + "_" + criterion
            win_performance_metrics, confusion_matrices, importances_array, roc_curve_array = \
                compute_performance_metrics(y_test,class_predictions, probas,importances, plot_name, plot_path)
                
            adjusted_probas = probas
            adjusted_class_predictions = class_predictions
            
            this_specificity = win_performance_metrics["specificity"]
            '''
            
            if win_performance_metrics["specificity"] < 0.85:
                # we need to increase the specificity

                increase_threshold = -0.001
                while(win_performance_metrics["specificity"] < 0.85):
                    threshold_adjustment = threshold_adjustment + increase_threshold
                    probaM, probaN = np.shape(probas)
                    for idx in range(probaM):
                        adjusted_probas[idx,0] = probas[idx,0] - threshold_adjustment
                        adjusted_probas[idx,1] = probas[idx,1] + threshold_adjustment
                        if adjusted_probas[idx,1] > 1.0:
                            adjusted_probas[idx,0] = 0.001
                            adjusted_probas[idx,1] = 0.999
                        if adjusted_probas[idx,1] >= 0.5:
                            adjusted_class_predictions[idx] = 1
                        else:
                            adjusted_class_predictions[idx] = -1
                            
                    win_performance_metrics, confusion_matrices, importances_array, roc_curve_array = \
                        compute_performance_metrics(y_test,class_predictions, adjusted_probas,importances, plot_name, plot_path)
                            
                    this_specificity = win_performance_metrics["specificity"]
                    
                    print("this_specificity = %.8f" % this_specificity)
                    print("threshold_adjustment = %.8f" % threshold_adjustment)
    

                    if this_specificity < 0.85:
                        print("this_specificity = %.6f" % this_specificity)
                    if this_specificity > 0.85:
                        print("this_specificity = %.6f" % this_specificity)
                    
                    
                    
            elif win_performance_metrics["specificity"] > 0.85:
                # we need to decrease the specificity
                increase_threshold = 0.00001

                while(win_performance_metrics["specificity"] > 0.85):
                    threshold_adjustment = threshold_adjustment + increase_threshold
                    probaM, probaN = np.shape(probas)
                    for idx in range(probaM):
                        adjusted_probas[idx,0] = probas[idx,0] - threshold_adjustment
                        adjusted_probas[idx,1] = probas[idx,1] + threshold_adjustment
                        if adjusted_probas[idx,1] > 1.0:
                            adjusted_probas[idx,0] = 0.001
                            adjusted_probas[idx,1] = 0.999
                        if adjusted_probas[idx,1] >= 0.5:
                            adjusted_class_predictions[idx] = 1
                        else:
                            adjusted_class_predictions[idx] = -1
                            
                    win_performance_metrics, confusion_matrices, importances_array, roc_curve_array = \
                        compute_performance_metrics(y_test,adjusted_class_predictions, adjusted_probas,importances, plot_name, plot_path)
                            
                    this_specificity = win_performance_metrics["specificity"]
                    print("this_specificity = %.8f" % this_specificity)
                    print("threshold_adjustment = %.8f" % threshold_adjustment)


                    if this_specificity < 0.85:
                        print("this_specificity = %.6f" % this_specificity)
                    if this_specificity > 0.85:
                        print("this_specificity = %.6f" % this_specificity)
                
            if win_performance_metrics["specificity"] < 0.85:
                # we need to increase the specificity

                increase_threshold = -0.00001
                while(win_performance_metrics["specificity"] < 0.85):
                    threshold_adjustment = threshold_adjustment + increase_threshold
                    probaM, probaN = np.shape(probas)
                    for idx in range(probaM):
                        adjusted_probas[idx,0] = probas[idx,0] - threshold_adjustment
                        adjusted_probas[idx,1] = probas[idx,1] + threshold_adjustment
                        if adjusted_probas[idx,1] > 1.0:
                            adjusted_probas[idx,0] = 0.001
                            adjusted_probas[idx,1] = 0.999
                        if adjusted_probas[idx,1] >= 0.5:
                            adjusted_class_predictions[idx] = 1
                        else:
                            adjusted_class_predictions[idx] = -1
                            
                    win_performance_metrics, confusion_matrices, importances_array, roc_curve_array = \
                        compute_performance_metrics(y_test,class_predictions, adjusted_probas,importances, plot_name, plot_path)
                            
                    this_specificity = win_performance_metrics["specificity"]
                    
                    print("this_specificity = %.8f" % this_specificity)
                    print("threshold_adjustment = %.8f" % threshold_adjustment)
    

                    if this_specificity < 0.85:
                        print("this_specificity = %.6f" % this_specificity)
                    if this_specificity > 0.85:
                        print("this_specificity = %.6f" % this_specificity)
                        break
                    
                    
                    
            elif win_performance_metrics["specificity"] > 0.85:
                # we need to decrease the specificity
                increase_threshold = 0.0001

                while(win_performance_metrics["specificity"] > 0.85):
                    threshold_adjustment = threshold_adjustment + increase_threshold
                    probaM, probaN = np.shape(probas)
                    for idx in range(probaM):
                        adjusted_probas[idx,0] = probas[idx,0] - threshold_adjustment
                        adjusted_probas[idx,1] = probas[idx,1] + threshold_adjustment
                        if adjusted_probas[idx,1] > 1.0:
                            adjusted_probas[idx,0] = 0.001
                            adjusted_probas[idx,1] = 0.999
                        if probas[idx,1] >= 0.5:
                            adjusted_class_predictions[idx] = 1
                        else:
                            adjusted_class_predictions[idx] = -1
                            
                    win_performance_metrics, confusion_matrices, importances_array, roc_curve_array = \
                        compute_performance_metrics(y_test,adjusted_class_predictions, adjusted_probas,importances, plot_name, plot_path)
                            
                    this_specificity = win_performance_metrics["specificity"]
                    print("this_specificity = %.8f" % this_specificity)
                    print("threshold_adjustment = %.8f" % threshold_adjustment)


                    if this_specificity < 0.85:
                        print("this_specificity = %.6f" % this_specificity)
                        break
                    if this_specificity > 0.85:
                        print("this_specificity = %.6f" % this_specificity)
                        
           '''           
            probaM, probaN = np.shape(probas)
            for idx in range(probaM):
                adjusted_probas[idx,0] = probas[idx,0] - threshold_adjustment
                adjusted_probas[idx,1] = probas[idx,1] + threshold_adjustment
                if adjusted_probas[idx,1] > 1:
                    adjusted_probas[idx,0] = 0.001
                    adjusted_probas[idx,1] = 0.999
                if probas[idx,1] >= 0.5:
                    adjusted_class_predictions[idx] = 1
                else:
                    adjusted_class_predictions[idx] = -1
             
                    
            
            
            # compute performance metrics
            plot_name = train_file_name + "_train_" + test_file_name + "_" + np.str(window_size) + "_" + model + "_" + criterion
            win_performance_metrics, confusion_matrices, importances_array, roc_curve_array = \
                compute_performance_metrics(y_test,adjusted_class_predictions, adjusted_probas,importances, plot_name, plot_path)
            #win_performance_metrics['feature_importances'] = importances_output
                
            win_performance_metrics["threshold_adjustment"] = threshold_adjustment
            
            
            this_fold_name = test_file_name + "_" + np.str(window_size) + "_" + model + "_" + criterion
        
        
        
            
            csv_name = "performance_metrics_" + test_file_name  + "_window_size_" + np.str(window_size ) + "_" + model + "_" + criterion 

            
            csv_path = script_path + result_path + csv_name + ".csv"
            with open(csv_path,'wb') as f:
                w = csv.DictWriter(f,win_performance_metrics.keys())
                w.writeheader()
                w.writerow(win_performance_metrics)
                f.close()
                
            fmt = "%d, %d"
            csv_name = "confusion_matrix_" + test_file_name  + "_window_size_" + np.str(window_size ) + "_" + model + "_" + criterion 
            
            csv_path = script_path + result_path + csv_name + ".csv"
            np.savetxt(csv_path, confusion_matrices, fmt=fmt)
        
            fmt = "%.6f, %.6f"
            
            csv_name = "adjusted_probas_" + test_file_name  + "_window_size_" + np.str(window_size ) + "_" + model + "_" + criterion

            
            csv_path = script_path + result_path + csv_name + ".csv"
            np.savetxt(csv_path, probas, fmt=fmt)
            
            csv_name = "adjusted_predictions_" + test_file_name  + "_window_size_" + np.str(window_size ) + "_" + model + "_" + criterion
            fmt = "%d"
            csv_path = script_path + result_path + csv_name + ".csv"
            np.savetxt(csv_path, class_predictions, fmt=fmt)
            
            csv_name = "feature_importances_" + train_file_name  + "_window_size_" + np.str(window_size ) + "_" + model + "_" + criterion
            fmt = "%.6f, %.6f, %.6f"
            csv_path = script_path + result_path + csv_name + ".csv"
            np.savetxt(csv_path, importances_array, fmt=fmt)
            
            csv_name = "roc_curve_" + test_file_name  + "_window_size_" + np.str(window_size ) + "_" + model + "_" + criterion 
            fmt = "%.6f, %.6f, %.6f"
            csv_path = script_path + result_path + csv_name + ".csv"
            np.savetxt(csv_path, roc_curve_array, fmt=fmt)    
        
               
            # clean up
            print("garbage collecting")
            X_test = []
            y_test = []
            gc.collect()
            
            
        
        
        
        print
        master_elapsed_time = time.time() - master_start_time
        print "Total time = %f" % master_elapsed_time
        
        progress_file.close()
